{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, json, numpy, os, pandas, requests, statsmodels.api\n",
    "\n",
    "RAW_GDT_DATA = '..\\\\data\\\\raw\\\\gdt'\n",
    "PROCESSED_DATA = '..\\\\data\\\\processed'\n",
    "\n",
    "def get_latest_key():\n",
    "    response = requests.get(\"https://s3.amazonaws.com/www-production.globaldairytrade.info/results/latest.json\")\n",
    "    return response.json()['latestEvent']\n",
    "    \n",
    "def get_product_group_result(key):\n",
    "    url = \"https://s3.amazonaws.com/www-production.globaldairytrade.info/results/\" + \\\n",
    "    key + \"/product_groups_summary.json\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "def get_event_summary(key):\n",
    "    url = \"https://s3.amazonaws.com/www-production.globaldairytrade.info/results/\" + \\\n",
    "    key + \"/event_summary.json\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "def get_results(key):\n",
    "    filename = RAW_GDT_DATA + '\\\\' + key + '.json'\n",
    "    if not (os.path.exists(filename) and os.path.isfile(filename)):\n",
    "        data = get_product_group_result(key)\n",
    "        data['event_summary'] = get_event_summary(key)\n",
    "        data['key'] = key\n",
    "        with open(filename, 'w') as io:\n",
    "            json.dump(data, io)\n",
    "        return data\n",
    "    return\n",
    "\n",
    "def get_latest_results():\n",
    "    key = get_latest_key()\n",
    "    return get_results(key)\n",
    "\n",
    "def rebuild_processed_gdt_events():\n",
    "    old_df = pandas.read_csv(RAW_GDT_DATA + '\\\\events.csv')\n",
    "    old_df.sort_values('trading_event')\n",
    "    event_results = []\n",
    "    for filename in os.listdir(RAW_GDT_DATA):\n",
    "        if filename[-5:] == \".json\":\n",
    "            if filename == 'nzx_settlements.json':\n",
    "                continue\n",
    "            with open(RAW_GDT_DATA + '\\\\' + filename, 'r') as io:\n",
    "                data = json.load(io)\n",
    "                date = datetime.datetime.strptime(\n",
    "                    data['event_summary']['EventSummary']['EventDate'],\n",
    "                    '%B %d, %Y %H:%M:%S')\n",
    "                event = {\n",
    "                    'trading_event': int(float(data['event_summary']['EventSummary']['EventNumber'])),\n",
    "                    'date': date.strftime('%Y-%m-%d')\n",
    "                }\n",
    "                for res in data['ProductGroups']['ProductGroupResult']:\n",
    "                    if 'AverageWinningPrice' in res:\n",
    "                        event[res['ProductGroupCode']] = res['AverageWinningPrice']\n",
    "                    else:\n",
    "                        event[res['ProductGroupCode']] = res['AveragePublishedPrice']\n",
    "                event_results.append(event)\n",
    "    event_results.sort(key= lambda x: x['trading_event'])\n",
    "    with open(PROCESSED_DATA + '\\\\gdt_events.csv', 'w') as io:\n",
    "        io.write('trading_event,date,amf,bmp,but,smp,wmp\\n')\n",
    "        first_new_event = event_results[0]['trading_event']\n",
    "        for row in old_df.iterrows():\n",
    "            if row[1]['trading_event'] >= first_new_event:\n",
    "                break\n",
    "            else:\n",
    "                date = datetime.datetime.strptime(row[1]['date'], '%d/%m/%Y')\n",
    "                io.write(str(row[1]['trading_event']) + ',' + date.strftime('%Y-%m-%d'))\n",
    "                for key in ['amf', 'bmp', 'but', 'smp', 'smp']:\n",
    "                    value = row[1][key]\n",
    "                    if pandas.isnull(value):\n",
    "                        io.write(',')\n",
    "                    else:\n",
    "                        io.write(',' + str(round(row[1][key])))\n",
    "                io.write('\\n')\n",
    "        for event in event_results:\n",
    "            io.write(str(event['trading_event']) + ',' + event['date'])\n",
    "            for key in ['AMF', 'BMP', 'Butter', 'SMP', 'WMP']:\n",
    "                io.write(',' + event[key])\n",
    "            io.write('\\n')\n",
    "            \n",
    "def impute_missing_gdt_events():\n",
    "    gdt_events = pandas.read_csv(PROCESSED_DATA + '\\\\gdt_events.csv')\n",
    "    # First imputation pass:\n",
    "    # If we are just missing a value for one week, average the values\n",
    "    # of the week before and the week after.\n",
    "    for key in ['amf', 'bmp', 'but', 'smp', 'wmp']:\n",
    "        for row in range(1, len(gdt_events['trading_event']) - 1):\n",
    "            if pandas.isnull(gdt_events[key][row]):\n",
    "                if not (pandas.isnull(gdt_events[key][row-1]) or pandas.isnull(gdt_events[key][row+1])):\n",
    "                    gdt_events.loc[row, key] = 0.5 * (gdt_events[key][row-1] + gdt_events[key][row+1])\n",
    "\n",
    "    for key in ['amf', 'smp', 'wmp']:\n",
    "        if any([pandas.isnull(x) for x in gdt_events['amf']]):\n",
    "            raise(Exception('Cannot impute as ' + key + ' contains a NaN.'))\n",
    "\n",
    "    # Second imputation pass:\n",
    "    # Impute bmp based on a linear regression of amf, smp, and wmp.\n",
    "    training_df = gdt_events.dropna()\n",
    "    features = pandas.DataFrame(training_df, columns=['amf', 'smp', 'wmp'])\n",
    "    for imputation_key in ['bmp', 'but']:\n",
    "        target = pandas.DataFrame(training_df, columns=[imputation_key])\n",
    "        model = statsmodels.api.OLS(target, features).fit()\n",
    "        for row in range(gdt_events.shape[0]):\n",
    "            if pandas.isnull(gdt_events.loc[row, imputation_key]):\n",
    "                row_df = gdt_events.loc[[row], ['amf', 'smp', 'wmp']]\n",
    "                gdt_events.loc[row, imputation_key] = float(model.predict(row_df))\n",
    "    for key in ['amf', 'bmp', 'but', 'smp', 'wmp']:\n",
    "        gdt_events[key] = [round(value) for value in gdt_events[key]]\n",
    "    gdt_events.sort_values('trading_event')\n",
    "    gdt_events.to_csv(PROCESSED_DATA + '\\\\gdt_events.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    calculate_average_sales_curve()\n",
    "    \n",
    "Calculate the average sales curve over 14 months. \n",
    "Can be run infrequently, e.g., whenever \n",
    "monthly_sales_contracts.csv is updated.\n",
    "\"\"\"\n",
    "def calculate_average_sales_curve():\n",
    "    sales = pandas.read_csv('..\\\\data\\\\raw\\\\fonterra\\\\monthly_sales_contracts.csv')\n",
    "    for col in range(1, sales.shape[1]):\n",
    "        for row in range(sales.shape[0]):\n",
    "            sales.iloc[row, col] /= sales.iloc[-1, col]\n",
    "    cumulative_sales_curve = [numpy.mean(sales.iloc[row, 1:]) for row in range(sales.shape[0])]\n",
    "    sales_curve = [cumulative_sales_curve[0]]\n",
    "    for i in range(1, len(cumulative_sales_curve)):\n",
    "        sales_curve.append(cumulative_sales_curve[i] - cumulative_sales_curve[i-1])\n",
    "    with open('..\\\\data\\\\processed\\\\monthly_sales_curve.json', 'w') as io:\n",
    "        json.dump([sales_curve[i] / 2 for i in range(len(sales_curve)) for j in range(2)], io)\n",
    "\n",
    "def get_average_sales_curve():\n",
    "    with open('..\\\\data\\\\processed\\\\monthly_sales_curve.json', 'r') as io:\n",
    "        return json.load(io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    calculate_average_product_mix_by_quarter()\n",
    "    \n",
    "Note that the output will weight more than the input milk\n",
    "due to the addition of water.\n",
    "\n",
    "Can be run infrequently, e.g., whenever quarterly_production.csv\n",
    "is updated.\n",
    "\"\"\"\n",
    "def calculate_average_product_mix_by_quarter():\n",
    "    production = pandas.read_csv('..\\\\data\\\\raw\\\\fonterra\\\\quarterly_production.csv')\n",
    "    for key in ['WMP', 'SMP', 'BUT', 'AMF', 'BMP']:\n",
    "        for row in range(production.shape[0]):\n",
    "            production.loc[row, key] /= production.loc[row, 'Supply']\n",
    "    production\n",
    "    months = ['jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may']\n",
    "    quarterly_production = pandas.DataFrame(data = {'quarter': [1,2,3,4]})\n",
    "    for key in ['WMP', 'SMP', 'BUT', 'AMF', 'BMP']:\n",
    "        quarterly_production[key] = [\n",
    "            numpy.mean(production[production['Period'] == quarter][key])\n",
    "            for quarter in range(1, 5)]\n",
    "    quarterly_production[['AMF', 'BMP', 'BUT', 'SMP', 'WMP']].to_csv('..\\\\data\\\\processed\\\\product_mix.csv', index=False)\n",
    "\n",
    "def get_product_mix():\n",
    "    product_mix = pandas.read_csv('..\\\\data\\\\processed\\\\product_mix.csv').values\n",
    "    product_mix = [product_mix[i] for i in range(4) for j in range(6)]\n",
    "    for i in range(4):\n",
    "        product_mix.append(product_mix[0])\n",
    "    return product_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Script to get all possible historical data.\n",
    "#\n",
    "# response = requests.get('https://s3.amazonaws.com/www-production.globaldairytrade.info/results/55331680-e829-41fa-b581-a09e593096d1/price_indices_ten_years.json')\n",
    "# data = response.json()\n",
    "# data['PriceIndicesTenYears']['Events']['EventDetails'][0]\n",
    "# events = []\n",
    "# for event in data['PriceIndicesTenYears']['Events']['EventDetails']:\n",
    "#     events.append({\n",
    "#         'number': event['EventNumber'],\n",
    "#         'date': event['EventDate'],\n",
    "#         'guid': event['EventGUID']\n",
    "#     })\n",
    "#     try:\n",
    "#         get_results(event['guid'])\n",
    "#     except:\n",
    "#         print(event['number'])\n",
    "\n",
    "# To be run daily:\n",
    "get_latest_results()\n",
    "rebuild_processed_gdt_events()\n",
    "impute_missing_gdt_events()\n",
    "\n",
    "calculate_average_sales_curve()\n",
    "calculate_average_product_mix_by_quarter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_log(data):\n",
    "    log_data = data[['amf', 'bmp', 'but', 'smp', 'wmp']]\n",
    "    for key in ['amf', 'bmp', 'but', 'smp', 'wmp']:\n",
    "        log_data.loc[:, key] = numpy.log(log_data[key])\n",
    "    return log_data\n",
    "\n",
    "def construct_gdt_model():\n",
    "    data = pandas.read_csv('..\\\\data\\\\processed\\\\gdt_events.csv')\n",
    "    data.loc[:, 'date'] = pandas.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "    data.sort_values('date')\n",
    "    log_data = to_log(data[['amf', 'bmp', 'but', 'smp', 'wmp']])\n",
    "    model = statsmodels.tsa.api.VAR(log_data)\n",
    "    data = data.set_index('date')\n",
    "    return model.fit(2), data\n",
    "\n",
    "def simulate_var(model, data, num_steps):\n",
    "    # x(t) = exp(A * log(x(t-1)) + B * log(x(t-2)) + c + \\varepsilon)\n",
    "    B = model.params.values[6:11]\n",
    "    A = model.params.values[1:6]\n",
    "    c = model.params.values[0]\n",
    "    noise = model.resid.values\n",
    "    output = data.copy()\n",
    "    for step in range(num_steps):\n",
    "        noise_index = numpy.random.choice(range(len(model.resid.values)))        \n",
    "        output = numpy.vstack(\n",
    "            (output,\n",
    "            numpy.exp(\n",
    "                # x'A rather than Ax because numpy is stupid.\n",
    "                numpy.matmul(numpy.log(output[-1]), A) + \n",
    "                numpy.matmul(numpy.log(output[-2]), B) +\n",
    "                c +\n",
    "                noise[noise_index])\n",
    "            )\n",
    "        )\n",
    "    return output\n",
    "\n",
    "def simulate_gdt(model, data, sales_curve, product_mix):\n",
    "    steps_remaining = 28 - data.shape[0]\n",
    "    forecast_data = simulate_var(model, data, steps_remaining)\n",
    "    gdt_value = 0.0\n",
    "    auctions = []\n",
    "    for trading_event in range(2, 28):\n",
    "        auction_value = numpy.dot(product_mix[trading_event - 2], forecast_data[trading_event])\n",
    "        gdt_value += sales_curve[trading_event - 2] * auction_value\n",
    "        auctions.append(auction_value)\n",
    "    return gdt_value / 1000, auctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_model(run_date = datetime.datetime.now().strftime('%Y-%m-%d')):\n",
    "    print('Conduction simulation run: %s' % run_date)\n",
    "    model, data = construct_gdt_model()\n",
    "    data = data.drop(['trading_event'], 1)\n",
    "    input_data = data[('2018-05-01' <= data.index) & (data.index <= run_date)].values\n",
    "    sales_curve = get_average_sales_curve()\n",
    "    product_mix = get_product_mix()\n",
    "\n",
    "    number_simulations = 1000\n",
    "    nzd_earnings = []\n",
    "    usd_simulations = []\n",
    "    fx_simulations = []\n",
    "    cost_simulations = []\n",
    "    auctions = []\n",
    "    for simulation in range(number_simulations):\n",
    "        usd_revenue, auction = simulate_gdt(model, input_data, sales_curve, product_mix)\n",
    "        auctions.append(auction)\n",
    "        usd_simulations.append(usd_revenue)\n",
    "        FX = numpy.random.uniform(0.655, 0.687)\n",
    "        fx_simulations.append(FX)\n",
    "        cost = numpy.random.uniform(1.88, 1.98)\n",
    "        cost_simulations.append(cost)\n",
    "        nzd_earnings.append(usd_revenue / FX - cost)\n",
    "    with open('..\\\\data\\\\models\\\\' + run_date + '.json', 'w') as io:\n",
    "        json.dump({\n",
    "            'usd_simulations': usd_simulations,\n",
    "            'fx_simulations': fx_simulations,\n",
    "            'cost_simulations': cost_simulations,\n",
    "            'nzd_earnings': nzd_earnings\n",
    "        }, io)\n",
    "    with open('..\\\\docs\\\\forecasts.json', 'r') as io:\n",
    "        json_str = io.read()\n",
    "        forecasts = json.loads(json_str[json_str.find('['):])\n",
    "        forecasts.append({\n",
    "            'date': run_date,\n",
    "            'model_version': MODEL_VERSION,\n",
    "            '10%': round(numpy.percentile(nzd_earnings, 10), 2),\n",
    "            '50%': round(numpy.percentile(nzd_earnings, 50), 2),\n",
    "            '90%': round(numpy.percentile(nzd_earnings, 90), 2)\n",
    "        })\n",
    "    with open('..\\\\docs\\\\forecasts.json', 'w') as io:\n",
    "        io.write('season_2018_19 = ')\n",
    "        io.write(json.dumps(forecasts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conduction simulation run: 2018-06-01\n",
      "Conduction simulation run: 2018-06-05\n",
      "Conduction simulation run: 2018-06-19\n",
      "Conduction simulation run: 2018-07-03\n",
      "Conduction simulation run: 2018-07-17\n",
      "Conduction simulation run: 2018-08-07\n",
      "Conduction simulation run: 2018-08-21\n",
      "Conduction simulation run: 2018-09-04\n",
      "Conduction simulation run: 2018-09-18\n"
     ]
    }
   ],
   "source": [
    "for run_date in [\n",
    "    '2018-06-01',\n",
    "    '2018-06-05',\n",
    "    '2018-06-19',\n",
    "    '2018-07-03',\n",
    "    '2018-07-17',\n",
    "    '2018-08-07',\n",
    "    '2018-08-21',\n",
    "    '2018-09-04',\n",
    "    '2018-09-18'\n",
    "        ]:\n",
    "    simulate_model(run_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
